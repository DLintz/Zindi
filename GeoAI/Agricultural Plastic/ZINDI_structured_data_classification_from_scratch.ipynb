{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "j-3cY2hlkya0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9rJ7Lb6eBVB"
   },
   "source": [
    "# Structured data classification 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk28xBAQeBVC"
   },
   "source": [
    "### The dataset\n",
    "\n",
    "Here's the description of each feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr_V8TaieBVD"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FsORTOHSeBVD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TensorFlow is the only backend that supports string inputs.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CWLydOlDkya3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 2\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAd9EUgTeBVD"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's download the data and load it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IcWJV8KEeBVD"
   },
   "outputs": [],
   "source": [
    "Kenya = pd.read_csv(\"GeoAI/Agricultural Plastic/Data/Kenya_training.csv\")\n",
    "Spain =  pd.read_csv(\"GeoAI/Agricultural Plastic/Data/Spain_training.csv\")\n",
    "VNM =  pd.read_csv(\"GeoAI/Agricultural Plastic/Data/VNM_training.csv\")\n",
    "VNM.rename(columns={'Lon': 'lon', 'Lat': 'lat'}, inplace=True) # It's to allign with the other two sources.\n",
    "\n",
    "dataframe = pd.concat([Kenya, Spain, VNM], axis=0)\n",
    "\n",
    "dataframe = dataframe.drop(['ID'], axis=1)\n",
    "dataframe['TARGET'] = dataframe['TARGET']-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fH31rLR43Ee_"
   },
   "outputs": [],
   "source": [
    "cols = dataframe.columns\n",
    "y = dataframe.pop('TARGET')\n",
    "X = dataframe\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "dataframe_test = pd.concat([X_test, y_test], axis=1)\n",
    "dataframe_test.columns = cols\n",
    "dataframe = pd.concat([X_train, y_train], axis=1)\n",
    "dataframe.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoEi90y0eBVE"
   },
   "source": [
    "The dataset includes 2825 samples with 18 columns per sample (13 features, plus the target\n",
    "label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4E8zGzNeBVE",
    "outputId": "2bde3b1a-8e55-47ae-adc3-c21787dcad35"
   },
   "outputs": [],
   "source": [
    "#print(dataframe.shape)\n",
    "#print(dataframe.columns)\n",
    "#dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EZodfmLeBVE"
   },
   "source": [
    "Here's a preview of a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceNMm06SeBVF",
    "outputId": "c2f39357-dfaa-4fc6-949f-4f0d3f62a922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1163 samples for training and 378 for validation\n"
     ]
    }
   ],
   "source": [
    "val_dataframe = dataframe.sample(frac=0.2, random_state=2)\n",
    "train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "print(\n",
    "    f\"Using {len(train_dataframe)} samples for training \"\n",
    "    f\"and {len(val_dataframe)} for validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-NE0S9SeBVF"
   },
   "source": [
    "Let's generate `tf.data.Dataset` objects for each dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EdiQs5BWeBVF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"TARGET\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmykWW5ZeBVF"
   },
   "source": [
    "Each `Dataset` yields a tuple `(input, target)` where `input` is a dictionary of features\n",
    "and `target` is the value `0` or `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9Ycvd62eBVF",
    "outputId": "46b1cd86-66c1-490d-b53c-0633d6811257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'lon': <tf.Tensor: shape=(), dtype=float64, numpy=37.30357524>, 'lat': <tf.Tensor: shape=(), dtype=float64, numpy=0.096344314>, 'blue_p50': <tf.Tensor: shape=(), dtype=float64, numpy=2708.0>, 'green_p50': <tf.Tensor: shape=(), dtype=float64, numpy=2882.0>, 'nir_p50': <tf.Tensor: shape=(), dtype=float64, numpy=5196.0>, 'nira_p50': <tf.Tensor: shape=(), dtype=float64, numpy=5631.0>, 're1_p50': <tf.Tensor: shape=(), dtype=float64, numpy=3270.0>, 're2_p50': <tf.Tensor: shape=(), dtype=float64, numpy=5025.0>, 're3_p50': <tf.Tensor: shape=(), dtype=float64, numpy=5508.0>, 'red_p50': <tf.Tensor: shape=(), dtype=float64, numpy=2782.0>, 'swir1_p50': <tf.Tensor: shape=(), dtype=float64, numpy=3969.0>, 'swir2_p50': <tf.Tensor: shape=(), dtype=float64, numpy=2853.0>, 'VV_p50': <tf.Tensor: shape=(), dtype=float64, numpy=-7.775951>, 'VH_p50': <tf.Tensor: shape=(), dtype=float64, numpy=-13.336234>}\n",
      "Target: tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 14:24:39.206235: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq5CbyAneBVF"
   },
   "source": [
    "Let's batch the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BmA4ahKreBVF"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDAzKm5ZeBVF"
   },
   "source": [
    "## Feature preprocessing with Keras layers\n",
    "\n",
    "\n",
    "All features are continuous numerical features:\n",
    "\n",
    "For each of these features, we will use a `Normalization()` layer to make sure the mean\n",
    "of each feature is 0 and its standard deviation is 1.\n",
    "\n",
    "Below, we define a utility function to do that operation:\n",
    "\n",
    "- `encode_numerical_feature` to apply featurewise normalization to numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Ql6vYlbpeBVF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = layers.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD9kq8_xeBVF"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "With this done, we can create our end-to-end model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckV2BfdIJj-P"
   },
   "source": [
    "### Adaptative Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PtSTXvpWJi-6"
   },
   "outputs": [],
   "source": [
    "class AdaptiveLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, decay_rate):\n",
    "        self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n",
    "        self.decay_steps = tf.cast(decay_steps, tf.float32)\n",
    "        self.decay_rate = tf.cast(decay_rate, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  # Ensure step is also float32\n",
    "        return self.initial_learning_rate * tf.math.pow(self.decay_rate, (step / self.decay_steps))\n",
    "\n",
    "# Example usage\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.96\n",
    "\n",
    "lr_schedule = AdaptiveLearningRateScheduler(initial_learning_rate, decay_steps, decay_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',    # Metric to monitor\n",
    "    patience=1000,           # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ymj2a0BveBVF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 14:28:50.454382: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    " # Numerical features\n",
    "lon = keras.Input(shape=(1,), name=\"lon\")\n",
    "lat = keras.Input(shape=(1,), name=\"lat\")\n",
    "blue_p50 = keras.Input(shape=(1,), name=\"blue_p50\")\n",
    "green_p50 = keras.Input(shape=(1,), name=\"green_p50\")\n",
    "nir_p50 = keras.Input(shape=(1,), name=\"nir_p50\")\n",
    "nira_p50 = keras.Input(shape=(1,), name=\"nira_p50\")\n",
    "re1_p50 = keras.Input(shape=(1,), name=\"re1_p50\")\n",
    "re2_p50 = keras.Input(shape=(1,), name=\"re2_p50\")\n",
    "re3_p50 = keras.Input(shape=(1,), name=\"re3_p50\")\n",
    "red_p50 = keras.Input(shape=(1,), name=\"red_p50\")\n",
    "swir1_p50 = keras.Input(shape=(1,), name=\"swir1_p50\")\n",
    "swir2_p50 = keras.Input(shape=(1,), name=\"swir2_p50\")\n",
    "VV_p50 = keras.Input(shape=(1,), name=\"VV_p50\")\n",
    "VH_p50 = keras.Input(shape=(1,), name=\"VH_p50\")\n",
    "\n",
    "all_inputs = [\n",
    "    lon,\n",
    "    lat,\n",
    "    blue_p50,\n",
    "    green_p50,\n",
    "    nir_p50,\n",
    "    nira_p50,\n",
    "    re1_p50,\n",
    "    re2_p50,\n",
    "    re3_p50,\n",
    "    red_p50,\n",
    "    swir1_p50,\n",
    "    swir2_p50,\n",
    "    VV_p50,\n",
    "    VH_p50\n",
    "]\n",
    "\n",
    "# Integer categorical features\n",
    "lon_encoded = encode_numerical_feature(lon, \"lon\", train_ds)\n",
    "lat_encoded = encode_numerical_feature(lat, \"lat\", train_ds)\n",
    "blue_p50_encoded = encode_numerical_feature(blue_p50, \"blue_p50\", train_ds)\n",
    "green_p50_encoded = encode_numerical_feature(green_p50, \"green_p50\", train_ds)\n",
    "nir_p50_encoded = encode_numerical_feature(nir_p50, \"nir_p50\", train_ds)\n",
    "nira_p50_encoded = encode_numerical_feature(nira_p50, \"nira_p50\", train_ds)\n",
    "re1_p50_encoded = encode_numerical_feature(re1_p50, \"re1_p50\", train_ds)\n",
    "re2_p50_encoded = encode_numerical_feature(re2_p50, \"re2_p50\", train_ds)\n",
    "re3_p50_encoded = encode_numerical_feature(re3_p50, \"re3_p50\", train_ds)\n",
    "red_p50_encoded = encode_numerical_feature(red_p50, \"red_p50\", train_ds)\n",
    "swir1_p50_encoded = encode_numerical_feature(swir1_p50, \"swir1_p50\", train_ds)\n",
    "swir2_p50_encoded = encode_numerical_feature(swir2_p50, \"swir2_p50\", train_ds)\n",
    "VV_p50_encoded = encode_numerical_feature(VV_p50, \"VV_p50\", train_ds)\n",
    "VH_p50_encoded = encode_numerical_feature(VH_p50, \"VH_p50\", train_ds)\n",
    "\n",
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        lon_encoded,\n",
    "        lat_encoded,\n",
    "        blue_p50_encoded,\n",
    "        green_p50_encoded,\n",
    "        nir_p50_encoded,\n",
    "        nira_p50_encoded,\n",
    "        re1_p50_encoded,\n",
    "        re2_p50_encoded,\n",
    "        re3_p50_encoded,\n",
    "        red_p50_encoded,\n",
    "        swir1_p50_encoded,\n",
    "        swir2_p50_encoded,\n",
    "        VV_p50_encoded,\n",
    "        VH_p50\n",
    "    ]\n",
    ")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x2 = layers.Dropout(0.5)(x2)\n",
    "x3 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x3 = layers.Dropout(0.5)(x3)\n",
    "x4 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x4 = layers.Dropout(0.5)(x4)\n",
    "x5 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x5 = layers.Dropout(0.5)(x5)\n",
    "x6 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x6 = layers.Dropout(0.5)(x6)\n",
    "x7 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x7 = layers.Dropout(0.5)(x7)\n",
    "x8 = layers.Dense(64, activation=\"relu\")(all_features)\n",
    "x8 = layers.Dropout(0.5)(x3)\n",
    "\n",
    "x1 = layers.concatenate((x1,x2))\n",
    "x2 = layers.concatenate((x3,x4))\n",
    "x3 = layers.concatenate((x5,x6))\n",
    "x4 = layers.concatenate((x6,x7))\n",
    "\n",
    "x1 = layers.Dense(128, activation=\"relu\")(x1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "x2 = layers.Dense(128, activation=\"relu\")(x2)\n",
    "x2 = layers.Dropout(0.5)(x2)\n",
    "x3 = layers.Dense(128, activation=\"relu\")(x3)\n",
    "x3 = layers.Dropout(0.5)(x3)\n",
    "x4 = layers.Dense(128, activation=\"relu\")(x4)\n",
    "x4 = layers.Dropout(0.5)(x4)\n",
    "\n",
    "x1 = layers.concatenate((x1,x2))\n",
    "x2 = layers.concatenate((x3,x4))\n",
    "\n",
    "x1 = layers.Dense(256, activation=\"relu\")(x1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "x2 = layers.Dense(256, activation=\"relu\")(x2)\n",
    "x2 = layers.Dropout(0.5)(x2)\n",
    "\n",
    "x = layers.concatenate((x1,x2))\n",
    "\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_HvO0WjlKlEE"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer,\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEE1MDI6eBVG"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwZY5K85eBVG",
    "outputId": "aea72e96-b127-4ec8-d138-7fece5fb4c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 - 4s - 97ms/step - accuracy: 0.5073 - loss: 190.0128 - val_accuracy: 0.5397 - val_loss: 16.9941\n",
      "Epoch 2/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.4884 - loss: 66.4899 - val_accuracy: 0.5397 - val_loss: 0.9965\n",
      "Epoch 3/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.4944 - loss: 28.2172 - val_accuracy: 0.4603 - val_loss: 1.6409\n",
      "Epoch 4/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.4979 - loss: 13.4325 - val_accuracy: 0.5397 - val_loss: 0.6837\n",
      "Epoch 5/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.5056 - loss: 6.8724 - val_accuracy: 0.5397 - val_loss: 0.6908\n",
      "Epoch 6/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.5099 - loss: 3.1976 - val_accuracy: 0.4603 - val_loss: 0.6941\n",
      "Epoch 7/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.5099 - loss: 2.2390 - val_accuracy: 0.4603 - val_loss: 0.6982\n",
      "Epoch 8/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.4927 - loss: 2.0689 - val_accuracy: 0.4841 - val_loss: 0.6934\n",
      "Epoch 9/10\n",
      "37/37 - 0s - 5ms/step - accuracy: 0.5039 - loss: 1.2644 - val_accuracy: 0.5238 - val_loss: 0.6921\n",
      "Epoch 10/10\n",
      "37/37 - 0s - 8ms/step - accuracy: 0.4996 - loss: 1.3602 - val_accuracy: 0.4603 - val_loss: 0.6952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x758cec594040>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[early_stopping], verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rFoNqK27OXa3"
   },
   "source": [
    "model.save('StructData_Model0.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QWLXxzCeBVG"
   },
   "source": [
    "## Inference on new data\n",
    "\n",
    "To get a prediction for a new sample, you can simply call `model.predict()`. There are\n",
    "just two things you need to do:\n",
    "\n",
    "1. wrap scalars into a list so as to have a batch dimension (models only process batches\n",
    "of data, not single samples)\n",
    "2. Call `convert_to_tensor` on each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XejVoyJN4_cn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('StructData_Model0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Dh_acIa5DQ3"
   },
   "source": [
    "sample = {\n",
    "    \"age\": 60,\n",
    "    \"sex\": 1,\n",
    "    \"cp\": 1,\n",
    "    \"trestbps\": 145,\n",
    "    \"chol\": 233,\n",
    "    \"fbs\": 1,\n",
    "    \"restecg\": 2,\n",
    "    \"thalach\": 150,\n",
    "    \"exang\": 0,\n",
    "    \"oldpeak\": 2.3,\n",
    "    \"slope\": 3,\n",
    "    \"ca\": 0,\n",
    "    \"thal\": \"fixed\",\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "predictions = model.predict(input_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOlj8k3s6JBX"
   },
   "source": [
    "### Test Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iklz1arz6Njd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = dataframe_test['TARGET']\n",
    "X = dict(dataframe_test.drop(['TARGET'], axis=1))\n",
    "\n",
    "pred = model.predict(X).astype(int)\n",
    "pred = pred.reshape(-1)\n",
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2k7F11yzfuE"
   },
   "outputs": [],
   "source": [
    "Kenya = pd.read_csv(\"Kenya_testing.csv\")\n",
    "Spain =  pd.read_csv(\"Spain_validation.csv\")\n",
    "VNM =  pd.read_csv(\"VNM_testing.csv\")\n",
    "VNM.rename(columns={'Lon': 'lon', 'Lat': 'lat'}, inplace=True) # It's to allign with the other two sources.\n",
    "\n",
    "dataframe = pd.concat([Kenya, Spain, VNM], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbVoqZLNwoV1"
   },
   "outputs": [],
   "source": [
    "#    dataframe = dataframe.copy()\n",
    "#    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n",
    "#    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "#    return ds\n",
    "ds = dict(dataframe)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDSy0hpPxixM",
    "outputId": "00962ca4-b029-4a46-8174-7b712f018af7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(ds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StwRwh-w0cY4",
    "outputId": "422ed4bb-944d-4b7e-8d25-95461b8c56cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWTvWJU25Ztk",
    "outputId": "6fde1648-0ea0-476b-f958-2b5f8e2e701c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "senWZyDt5bQ9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
